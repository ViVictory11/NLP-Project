{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260cf370",
   "metadata": {},
   "source": [
    "# IMDB Sentiment-Analyse: Naive Bayes vs. LSTM\n",
    "Dieses Notebook lädt das IMDB-Dataset aus dem Ordner `Data/` und trainiert zwei Klassifikatoren: einen klassischen Ansatz (TF-IDF + Naive Bayes) und ein Deep-Learning-Modell (LSTM). Zum Schluss vergleichen wir Metriken, Laufzeit und Interpretierbarkeit und geben Empfehlungen.\n",
    "\n",
    "Hinweis: Die Zellen sind so gestaltet, dass das Notebook auf einem typischen Studenten-Laptop lauffähig ist (begrenzte Vokabulargröße, wenige Epochen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports und Einstellungen\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Reproduzierbarkeit\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Zeige Versionen\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "print('tensorflow', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Daten laden (robustes Snippet)\n",
    "data_path = os.path.join('Data', 'IMDB Dataset.csv')\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f'Datei nicht gefunden: {data_path} - stelle sicher, dass sie im Ordner Data/ liegt')\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print('Shape:', df.shape)\n",
    "display(df.head(5))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d71224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: EDA - Klassenverteilung & Längenverteilung\n",
    "if 'sentiment' in df.columns and 'review' in df.columns:\n",
    "    display(df['sentiment'].value_counts())\n",
    "    sns.countplot(data=df, x='sentiment')\n",
    "    plt.title('Class distribution')\n",
    "    plt.show()\n",
    "    df['review_len'] = df['review'].apply(lambda x: len(str(x).split()))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(df['review_len'], bins=50)\n",
    "    plt.title('Review length distribution (words)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Erwarte Spalten `review` und `sentiment` im Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ba33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Preprocessing - clean_text Funktion\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # HTML entfernen\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # non-letters entfernen\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Sample Anwendung (erst nur auf kleinen Subset um Zeit zu sparen)\n",
    "df['clean_review'] = df['review'].astype(str).str[:500].apply(clean_text)  # safe apply\n",
    "display(df[['review','clean_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2489fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label-Encoding und train/test split (Section 4/6)\n",
    "label_map = {'positive':1, 'negative':0}\n",
    "df['label'] = df['sentiment'].map(label_map)\n",
    "X = df['clean_review']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "print('Train/Test sizes:', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2a858",
   "metadata": {},
   "source": [
    "### Modell 1: TF-IDF + Naive Bayes (klassischer Ansatz)\n",
    "Wir verwenden `TfidfVectorizer` + `MultinomialNB`. Dieser Ansatz ist schnell, gut interpretierbar (Top-Features) und für Bag-of-Words-Textklassifikation häufig sehr effektiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Metriken-Funktion\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "\n",
    "def plot_confusion(y_true, y_pred, labels=[0,1], title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d21634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Train Naive Bayes\n",
    "nb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1,2))),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "t0 = time.time()\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print(f'Training Naive Bayes took {t1-t0:.2f} sec')\n",
    "y_pred_nb = nb_pipeline.predict(X_test)\n",
    "metrics_nb = compute_metrics(y_test, y_pred_nb)\n",
    "print('Naive Bayes results:', metrics_nb)\n",
    "print('\n",
    "Classification report:\n",
    "', classification_report(y_test, y_pred_nb))\n",
    "plot_confusion(y_test, y_pred_nb, title='Naive Bayes Confusion Matrix')\n",
    "# Speichere Modell\n",
    "joblib.dump(nb_pipeline, 'nb_pipeline.joblib')\n",
    "print('NB pipeline saved to nb_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec8887",
   "metadata": {},
   "source": [
    "### Modell 2: LSTM (Deep Learning)\n",
    "Wir erstellen ein einfaches LSTM-Modell mit Embedding-Layer. Längere Trainingszeit und höhere Rechenkosten können auftreten; deshalb begrenzen wir Vokabulargröße und Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Tokenisierung und Sequenzvorbereitung\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_LEN = 200\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "print('Vocabulary size (approx):', min(MAX_NUM_WORDS, len(tokenizer.word_index)))\n",
    "print('X_train_pad shape:', X_train_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: LSTM Modellaufbau und Training\n",
    "EMBEDDING_DIM = 128\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "# train (keeping epochs small for student machines)\n",
    "t0 = time.time()\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=128, validation_split=0.1, callbacks=[es], verbose=1)\n",
    "t1 = time.time()\n",
    "print(f'LSTM training took {t1-t0:.2f} sec')\n",
    "# Speichere Modell und Tokenizer\n",
    "model.save('lstm_model.h5')\n",
    "joblib.dump(tokenizer, 'tokenizer.joblib')\n",
    "print('LSTM model and tokenizer saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f359d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Evaluation LSTM\n",
    "y_probs = model.predict(X_test_pad, batch_size=128)\n",
    "y_pred_lstm = (y_probs.flatten() >= 0.5).astype(int)\n",
    "metrics_lstm = compute_metrics(y_test, y_pred_lstm)\n",
    "print('LSTM results:', metrics_lstm)\n",
    "print('\n",
    "Classification report:\n",
    "', classification_report(y_test, y_pred_lstm))\n",
    "plot_confusion(y_test, y_pred_lstm, title='LSTM Confusion Matrix')\n",
    "# Plot Training History\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend(); plt.title('Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.legend(); plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a175a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Vergleich beider Modelle in einer Tabelle\n",
    "results = pd.DataFrame([\n",
    "    {'model':'NaiveBayes', **metrics_nb},\n",
    "    {'model':'LSTM', **metrics_lstm}\n",
    "])\n",
    "display(results)\n",
    "results.to_csv('model_comparison.csv', index=False)\n",
    "print('Vergleich gespeichert als model_comparison.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404173df",
   "metadata": {},
   "source": [
    "### Interpretation & Diskussion (kurz)\n",
    "- Naive Bayes ist sehr schnell, interpretiert durch Top-Features (TF-IDF Gewichtungen).\n",
    "- LSTM kann Kontext und Reihenfolge lernen, ist aber rechenaufwändiger und benötigt mehr Daten oder Regularisierung, um nicht zu überfitten.\n",
    "- Wenn NB ähnliche oder bessere Metriken liefert, ist der klassische Ansatz für Produktions-Einsätze oft ausreichend; andernfalls lohnt sich Fein-Tuning oder der Einsatz vortrainierter Embeddings/Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc47475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12: Top-Features aus NB anzeigen (Interpretierbarkeit)\n",
    "tfidf = nb_pipeline.named_steps['tfidf']\n",
    "nb = nb_pipeline.named_steps['nb']\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "class0_top = np.argsort(nb.feature_log_prob_[0])[-20:]\n",
    "class1_top = np.argsort(nb.feature_log_prob_[1])[-20:]\n",
    "print('Top features negative:')\n",
    "print(feature_names[class0_top])\n",
    "print('Top features positive:')\n",
    "print(feature_names[class1_top])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2084b",
   "metadata": {},
   "source": [
    "### Reproduzierbarkeit & Hinzufügungen\n",
    "- Modelle: `nb_pipeline.joblib`, `lstm_model.h5`, `tokenizer.joblib` wurden gespeichert.\n",
    "- Paketversionen oben ausgeben; für vollständige Reproduzierbarkeit siehe `requirements.txt`.\n",
    "- Für weitergehende Verbesserungen: Hyperparameter-Tuning (GridSearchCV für NB-Pipeline), pretrained embeddings (GloVe), oder Transformer-Modelle (z.B. DistilBERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86407816",
   "metadata": {},
   "source": [
    "## How to run\n",
    "1. Installiere Abhängigkeiten: `pip install -r requirements.txt`\n",
    "2. Öffne dieses Notebook in VS Code oder Jupyter und führe Zellen nacheinander aus.\n",
    "3. Falls NLTK-Stopwords nicht vorhanden sind, wird der Download in einer Zelle ausgeführt."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
